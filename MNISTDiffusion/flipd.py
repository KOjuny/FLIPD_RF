import torch
from torchvision.utils import save_image
from model import MNISTDiffusion
from utils import ExponentialMovingAverage
import math
import matplotlib.pyplot as plt
import numpy as np
import os


ckpt_path = "results/steps_00046900.pt"   # âœ… ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
n_samples = 1                             # âœ… ìƒì„±í•  ìƒ˜í”Œ ê°œìˆ˜
model_base_dim = 64                       # âœ… í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ base dim
timesteps = 1000                          # âœ… í•™ìŠµí•  ë•Œ ì‚¬ìš©í–ˆë˜ diffusion steps
no_clip = False                           # âœ… x_0 clipping ì“¸ì§€ ì—¬ë¶€
use_cpu = False                           # âœ… Trueë©´ CPU ì‚¬ìš©, Falseë©´ GPU ì‚¬ìš©
device = "cpu" if use_cpu else "cuda"

def load_model(ckpt_path, device, image_size=28, in_channels=1, model_base_dim=64, timesteps=1000):
    model = MNISTDiffusion(
        timesteps=timesteps,
        image_size=image_size,
        in_channels=in_channels,
        base_dim=model_base_dim,
        dim_mults=[2, 4]
    ).to(device)

    model_ema = ExponentialMovingAverage(model, device=device, decay=0.0)  # decayëŠ” ì¤‘ìš” X (ë¡œë“œí•  ê±°ë‹ˆê¹Œ)

    # checkpoint ë¡œë“œ
    ckpt = torch.load(ckpt_path, map_location=device)
    model.load_state_dict(ckpt["model"])
    model_ema.load_state_dict(ckpt["model_ema"])

    return model_ema

# ëª¨ë¸ ë¡œë“œ
model_ema = load_model(
    ckpt_path=ckpt_path,
    device=device,
    image_size=28,
    in_channels=1,
    model_base_dim=model_base_dim,
    timesteps=timesteps
)

model_ema.eval()

for i in range(1):
    # ìƒ˜í”Œ ìƒì„±
    samples, flipd, timestep = model_ema.module.sampling(
        n_samples=n_samples,
        clipped_reverse_diffusion=no_clip,
        device=device
    )

    # íŒŒì¼ ì¸ë±ìŠ¤ ë¬¸ìì—´ ìƒì„± (ì˜ˆ: 001)
    index_str = str(i + 1).zfill(3)

    # ğŸ”¹ ì´ë¯¸ì§€ ì €ì¥
    image_path = f"results/images/generated_samples_{index_str}.png"
    os.makedirs(os.path.dirname(image_path), exist_ok=True)
    save_image(samples, image_path, nrow=int(math.sqrt(n_samples)))
    print(f"âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ '{image_path}'ë¡œ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")

    # ğŸ”¹ FLIPD ê·¸ë˜í”„ ì €ì¥
    fig_path = f"results/figures/flipd_plot_{index_str}.png"
    os.makedirs(os.path.dirname(fig_path), exist_ok=True)

    plt.figure()
    plt.plot(timestep, flipd, label="FLIPD")

    # # ğŸ”¸ ìµœì†Œê°’ ê³„ì‚° ë° í‘œì‹œ
    # min_idx = int(np.argmin(flipd))
    # min_val = flipd[min_idx]
    # plt.scatter(min_idx, min_val, color='red', zorder=5)
    # plt.annotate(f"min={min_val:.4f}", xy=(min_idx, min_val),
    #              xytext=(min_idx + 2, min_val),
    #              arrowprops=dict(facecolor='red', shrink=0.05),
    #              fontsize=9, color='red')

    # ê·¸ë˜í”„ ì„¤ì •
    plt.title("Line Plot of FLIPD")
    plt.xlabel("Timestep")
    plt.ylabel("FLIPD Value")
    plt.grid(True)
    plt.legend()
    plt.xlim(0,1)
    plt.savefig(fig_path)
    plt.close()
    print(f"âœ… FLIPD ê·¸ë˜í”„ë¥¼ '{fig_path}'ë¡œ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")

# # ìƒ˜í”Œ ìƒì„±
# samples, flipd = model_ema.module.sampling(
#     n_samples=n_samples,
#     clipped_reverse_diffusion= not no_clip,
#     device=device
# )
# flipd = flipd[:-1]
# # ê²°ê³¼ ì €ì¥
# os.makedirs("results/images", exist_ok=True)
# save_image(samples, "results/images/generated_samples.png", nrow=int(math.sqrt(n_samples)))

# print(f"âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ 'results/images/generated_samples.png'ë¡œ ì €ì¥ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")


# # ì €ì¥í•  í´ë” ê²½ë¡œ
# output_dir = "results/figures"
# os.makedirs(output_dir, exist_ok=True)  # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±

# # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
# plt.plot(flipd)
# plt.title("Line Plot of List")
# plt.xlabel("Index")
# plt.ylabel("Value")
# plt.xlim()
# plt.grid(True)

# # ì €ì¥í•˜ê¸° (ì˜ˆ: figures/flipd_plot.png)
# output_path = os.path.join(output_dir, "flipd_plot.png")
# plt.savefig(output_path)